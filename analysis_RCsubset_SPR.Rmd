---
title: 'SAP Benchmark (SPR): RC subset'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lme4)
library(tidyverse)
library(ggplot2)
library(brms)
library(bayestestR)
```

### Load in data

```{r}

rt.data <- read.csv("./preprocessed_data/RelativeClauseSet.csv", header=TRUE) %>%
  # filter(ROI %in% c(-2,-1,0,1,2)) %>%
  filter(RT <= 7000) %>%
  filter(RT > 0) %>%
  rename(participant = MD5)

filler.data <- read.csv("./preprocessed_data/Fillers.csv", header = TRUE) %>%
  filter(RT <=7000) %>%
  filter(RT > 0) %>%
  rename(participant = MD5)

```


### Plotting the data

Lets start by plotting the mean RTs for words in the critical positions

```{r}

rt.data_summ <- rt.data %>%
  filter(WordPosition %in% c(2,3,4,5,6)) %>%
  group_by(WordPosition, Type) %>%
  summarise(mean_rt = mean(RT),
            se_rt = sd(RT)/sqrt(n())) %>%
  ungroup() 



ggplot(rt.data_summ, aes(x=WordPosition, y=mean_rt, colour=Type, shape=Type)) +
  geom_point() +
  geom_errorbar(aes(ymin=mean_rt - (2*se_rt), 
                    ymax = mean_rt + (2*se_rt)),
                width=.5,position=position_dodge(0.02)) + 
  scale_x_continuous(breaks = c(2,3,4,5,6),
                     labels = c("reporter", "that", "attacked \n the", "the \n senator", "senator \n attacked"))+
  labs(x = '', y = 'Mean RT (ms)')


## SRC: The girl that 

```

To ease the comparison of the same words with each other, we can align them. 

```{r}

rt.data_summ2 <- rt.data %>%
  group_by(ROI, Type) %>%
  summarise(mean_rt = mean(RT),
            se_rt = sd(RT)/sqrt(n()))

ggplot(rt.data_summ2, aes(x=ROI, y=mean_rt, colour=Type, shape=Type)) +
  geom_point() +
  geom_errorbar(aes(ymin=mean_rt - (2*se_rt), 
                    ymax = mean_rt + (2*se_rt)),
                width=.5,position=position_dodge(0.02)) + 
  scale_x_continuous(breaks = c(-2,-1,0,1,2),
                     labels = c("NOUN", "that", "VERB", "DET", "NOUN")) +
  labs(x = '', y = 'Mean RT (ms)')

## SRC: The girl that 

```

Even after alignment, the VERBs in both conditions are not directly comparable to each other -- since they occur in different positions. To account for the effect of word position, we can compute RTs with the effect of word position regressed out. 

### Specifying the dependent variable

```{r}

# position_fit_lmer <- lmer(RT ~ scale(WordPosition) + (1 + scale(WordPosition) | participant), filler.data)
position_fit_lmer_nocor <- lmer(RT ~ scale(WordPosition) + (1 + scale(WordPosition) || participant), filler.data)

position_fit_lm <- lm(RT ~ scale(WordPosition), filler.data)

#summary(position_fit_lmer)
summary(position_fit_lmer_nocor)
summary(position_fit_lm)

rt.data$wordpos_predrt <- predict(position_fit_lmer_nocor, rt.data)
rt.data$wordpos_predrt_lm <- predict(position_fit_lm, rt.data)

rt.data$corrected_rt <- rt.data$RT - rt.data$wordpos_predrt
rt.data$corrected_rt_lm <- rt.data$RT - rt.data$wordpos_predrt_lm

```

```{r}

ggplot(rt.data, aes(x=wordpos_predrt, y = wordpos_predrt_lm)) +
  geom_point()

ggplot(rt.data, aes(x=corrected_rt, y = corrected_rt_lm)) +
  geom_point()

ggplot(rt.data, aes(x=corrected_rt, y = RT)) +
  geom_point()
```


```{r}

rt.data_summ3 <- rt.data %>%
  group_by(ROI, Type) %>%
  summarise(mean_corrected_rt = mean(corrected_rt),
            se_corrected_rt = sd(corrected_rt)/sqrt(n()))

ggplot(rt.data_summ3, aes(x=ROI, y=mean_corrected_rt, colour=Type, shape=Type)) +
  geom_point() +
  geom_errorbar(aes(ymin=mean_corrected_rt - (2*se_corrected_rt), 
                    ymax = mean_corrected_rt + (2*se_corrected_rt)),
                width=.5,position=position_dodge(0.02)) + 
  scale_x_continuous(breaks = c(-2,-1,0,1,2),
                     labels = c("NOUN", "that", "VERB", "DET", "NOUN")) +
  labs(x = '', y = 'Mean corrected RT (ms)')

```

### Fitting mixed effects model

**Pre-registered analysis**
```{r}

verb_dat <- rt.data %>%
  filter(ROI == 0) %>%
  mutate(Type = factor(Type, levels = c('RC_Subj', 'RC_Obj')),
         Type_num = ifelse(Type == 'RC_Subj', 0, 1))

contrasts(verb_dat$Type)

## part intercept is 0 because we removed out this intercept through word pos correction
fit_verb_lmer <- lmer(corrected_rt ~ Type_num + (0 + Type_num || participant) + (1 + Type_num || item), data=verb_dat)

summary(fit_verb_lmer)

saveRDS(fit_verb_lmer, './saved_objects/fit_verb_lmer')


```

**Looking at other word positions**
```{r}
det_dat <- rt.data %>%
  filter(ROI == 1) %>%
  mutate(Type = factor(Type, levels = c('RC_Subj', 'RC_Obj')),
         Type_num = ifelse(Type == 'RC_Subj', 0, 1))

contrasts(det_dat$Type)

## part intercept is 0 because we removed out this intercept through word pos correction
fit_det_lmer <- lmer(corrected_rt ~ Type_num + (0 + Type_num | participant) + (1 + Type_num | item), data=det_dat)

summary(fit_det_lmer)

saveRDS(fit_det_lmer, './saved_objects/fit_det_lmer')



noun_dat <- rt.data %>%
  filter(ROI == 2) %>%
  mutate(Type = factor(Type, levels = c('RC_Subj', 'RC_Obj')),
         Type_num = ifelse(Type == 'RC_Subj', 0, 1))

contrasts(noun_dat$Type)

## part intercept is 0 because we removed out this intercept through word pos correction
fit_noun_lmer <- lmer(corrected_rt ~ Type_num + (0 + Type_num | participant) + (1 + Type_num | item), data=noun_dat)

summary(fit_noun_lmer)

saveRDS(fit_noun_lmer, './saved_objects/fit_noun_lmer')

```



### Fitting BRMS model

#### At the verb

```{r}
prior1 <- c(prior("normal(300,1000)", class = "Intercept"),
            prior("normal(0,150)", class = "b"),  
            prior("normal(0,200)", class = "sd"),    
            prior("normal(0,500)", class = "sigma"))

#Note brms automatically truncates the distributions for sd and sigma.

```

```{r, cache=TRUE}

# fit_verb_bayes <- brm(corrected_rt ~ Type_num + (0 + Type_num || participant) + (1 + Type_num || item),
#                   data=verb_dat,
#                   prior = prior1,
#                   cores = 4,
#                   iter = 6000,
#                   seed = 117
#                   )
# 
# saveRDS(fit_verb_bayes, './saved_objects/fit_verb_bayes_prior1')
# 
# summary(fit_verb_bayes)


fit_verb_bayes <- readRDS('./saved_objects/fit_verb_bayes_prior1')
```

```{r, cache=TRUE}

# fit_det_bayes <- brm(corrected_rt ~ Type_num + (0 + Type_num || participant) + (1 + Type_num || item),
#                   data=det_dat,
#                   prior = prior1,
#                   cores = 4,
#                   iter = 6000,
#                   seed = 117
#                   )
# 
# saveRDS(fit_det_bayes, './saved_objects/fit_det_bayes_prior1')
# 
# summary(fit_det_bayes)


fit_det_bayes <- readRDS('./saved_objects/fit_det_bayes_prior1')
```



```{r, cache=TRUE}

# fit_noun_bayes <- brm(corrected_rt ~ Type_num + (0 + Type_num || participant) + (1 + Type_num || item),
#                   data=noun_dat,
#                   prior = prior1,
#                   cores = 4,
#                   iter = 6000,
#                   seed = 117
#                   )
# 
# saveRDS(fit_noun_bayes, './saved_objects/fit_noun_bayes_prior1')
# 
# summary(fit_noun_bayes)


fit_noun_bayes <- readRDS('./saved_objects/fit_noun_bayes_prior1')
```




```{r}

source("util.R")

# pred_dat_verb <- reshape_item_dat(fit_verb_bayes, "item") %>%
#   mutate(ROI = 'Verb')
# pred_dat_det <- reshape_item_dat(fit_det_bayes, "item") %>%
#   mutate(ROI = 'Determiner')
# pred_dat_noun <- reshape_item_dat(fit_noun_bayes, "item")%>%
#   mutate(ROI = 'Noun')
# 
# pred_dat <- dplyr::bind_rows(pred_dat_verb, pred_dat_det, pred_dat_noun)
# 
# rm(pred_dat_verb, pred_dat_det, pred_dat_noun)
# 
# saveRDS(pred_dat, './saved_objects/rc_subset_pred_empirical')

pred_dat <- readRDS('./saved_objects/rc_subset_pred_empirical')

```


```{r}

by_construction <- pred_dat %>%
  mutate(diff = b_Type_num) %>%
  group_by(ROI) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]])

by_item <- pred_dat %>%
   mutate(diff = b_Type_num + r_Type_num) %>%
  group_by(item,ROI) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]])

#by_item_surprisalmerged <- merge_surprisal(rt.data,by_item,"RelativeClause")

```

```{r}
Plot_empirical_construction_level(by_construction,"RelativeClause")

by_item$coef <- 'RC'
Plot_itemwise_by_magnitude(by_item,"RelativeClause",ROI='Verb')
```

### Old plotting stuff

**Plot group level effects**
```{r}

fit1_samples_summ <- posterior_samples(fit1_bayes) %>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num) %>%
  select(RT_subj, RT_obj) %>%
  gather(key = 'cond', value = 'RT', RT_subj, RT_obj) %>%
  group_by(cond) %>%
  summarise(mean = mean(RT),
            lower = quantile(RT, 0.025)[[1]],
            upper = quantile(RT, 0.975)[[1]]) %>%
  mutate(region = 'Verb')

ggplot(fit1_samples_summ, aes(x=cond, y=mean, fill=cond)) +
  geom_bar(stat="identity", postion= position_dodge() ) + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper), 
                width=0.2) + 
  facet_wrap(~region)

```

**Plot item level effects**

```{r}

split_by_randomeffect <- function(fit) {
  post_samples <- posterior_samples(fit)
  
  intercept <- post_samples %>%
    select(matches('r_item.*Intercept]')) %>%
    mutate(sim = c(1:n())) %>%
    gather(key='key', 'item_intercept', matches('r_item.*Intercept]')) %>%
    mutate(item = gsub(".*?([0-9]+).*", "\\1", key))
  
  slope <- post_samples %>%
    select(matches('r_item.*Type_num]')) %>%
    gather(key='key', 'item_slope', matches('r_item.*Type_num]')) %>%
    mutate(item = gsub(".*?([0-9]+).*", "\\1", key)) %>%
    select(-key)
  
  fixed <-  post_samples %>%
    select('b_Intercept', 'b_Type_num') %>%
    mutate(sim = c(1:n()))
  
  combined <- cbind(intercept, slope)
  combined <- merge(combined, fixed, by='sim') 
  combined <- combined[, !duplicated(colnames(combined))]
  
  return(combined)
  
}


fit1_byitem <- split_by_randomeffect(fit1_bayes)%>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Verb')



```


```{r}

ggplot(fit1_byitem, aes(x=item, y=mean)) +
  geom_point() + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper), 
                width=0.2) 

# + 
#   facet_wrap(~region)

```

#### At other positions

```{r, cache=TRUE}

# fit2_bayes <- brm(corrected_rt ~ Type_num + (0 + Type_num | participant) + (1 + Type_num | item),
#                   data=det_dat,
#                   prior = prior1,
#                   cores = 4,
#                   iter = 4000,
#                   seed = 117
#                   )


# saveRDS(fit2_bayes, './saved_objects/fit2_bayes_prior1_rawrt')

fit2_bayes <- readRDS('./saved_objects/fit2_bayes_prior1_rawrt')


fit3_bayes <- brm(corrected_rt ~ Type_num + (0 + Type_num | participant) + (1 + Type_num | item),
                  data=noun_dat,
                  prior = prior1,
                  cores = 4,
                  iter = 4000,
                  seed = 117
                  )

saveRDS(fit3_bayes, './saved_objects/fit3_bayes_prior1_rawrt')

```


```{r, cache=TRUE}


fit2_samples_summ <- posterior_samples(fit2_bayes) %>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num)%>%
  select(RT_subj, RT_obj) %>%
  gather(key = 'cond', value = 'RT', RT_subj, RT_obj)%>%
  group_by(cond) %>%
  summarise(mean = mean(RT),
            lower = quantile(RT, 0.025)[[1]],
            upper = quantile(RT, 0.975)[[1]]) %>%
  mutate(region = "Determiner")

fit3_samples_summ <- posterior_samples(fit3_bayes) %>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num) %>%
  select(RT_subj, RT_obj) %>%
  gather(key = 'cond', value = 'RT', RT_subj, RT_obj) %>%
  group_by(cond) %>%
  summarise(mean = mean(RT),
            lower = quantile(RT, 0.025)[[1]],
            upper = quantile(RT, 0.975)[[1]]) %>%
  mutate(region = "Noun")


all_fit_samples_summ <- dplyr::bind_rows(fit1_samples_summ, fit2_samples_summ, fit3_samples_summ)

ggplot(all_fit_samples_summ, aes(x=cond, y=mean, fill=cond)) +
  geom_bar(stat="identity", postion= position_dodge() ) + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper), 
                width=0.2) + 
  facet_wrap(~region)


```


```{r, cache=TRUE}

fit2_byitem <- split_by_randomeffect(fit2_bayes)%>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Determiner')

fit3_byitem <- split_by_randomeffect(fit3_bayes) %>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Noun')


all_split <- dplyr::bind_rows(fit1_byitem, fit2_byitem, fit3_byitem) 

rm(fit1_byitem, fit2_byitem, fit3_byitem)

  
```


```{r}

ggplot(all_split, aes(x=item, y=mean)) +
  geom_point() + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper), 
                width=0.2) + 
  facet_wrap(~region)

```


### Correlating with Surprisal

#### Correlation with raw surprisals

```{r}

surp_files <- c('./Surprisals/items_orc.lstm.csv',
                './Surprisals/items_orc.gpt2.csv',
                './Surprisals/items_orc.rnng.csv')


surp_list <- list()

i <- 1
for(fname in surp_files){
  model_name <- strsplit(fname, '.', fixed=TRUE)[[1]][3]
  curr_surp <- read.csv(fname) %>%
    mutate(model = model_name,
           surprisal = ifelse(surprisal == -1, NA, surprisal),
           word_pos = word_pos + 1)

  surp_list[[i]] <- curr_surp
  i <- i +1
}


surps_rc <- dplyr::bind_rows(surp_list)

```

```{r}
nrow(rt.data)
rt.data.merged <- merge(x=rt.data, y=surps_rc, 
                      by.x=c("Sentence", "WordPosition"), by.y=c("Sentence", "word_pos"))

nrow(rt.data.merged)

surp.diff <- rt.data.merged %>%
  select(item, Type, surprisal, ROI, participant, model) %>%
  filter(ROI %in% c(0,1,2)) %>%
  mutate(region = ifelse(ROI == 0, 'Verb', ifelse(ROI == 1, 'Determiner', 'Noun'))) %>%
  group_by(item, Type, surprisal, region, model) %>%
  summarize(surprisal = mean(surprisal)) %>%   #should be the same anywya for all participants
  ungroup() %>%
  spread(key=Type, value = surprisal) %>%
  mutate(surp_diff = RC_Obj - RC_Subj) 
  


all_split_surp <- merge(all_split, surp.diff, by = c('region', 'item'))

nrow(surp.diff)
nrow(all_split) 
nrow(all_split_surp)  #figure out why this is not the same? 

```

```{r}

ggplot(all_split_surp, aes(y = mean, x = surp_diff)) + 
  geom_point() + 
  facet_grid(model~region) + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper), 
                width=0.2) + 
  labs(y = 'RT(ORC) - RT(SRC)', x = 'Surp(ORC) - Surp(SRC)') +
  geom_smooth(method = 'lm')


## I am not sure if this plot makes sense -- the RTs are generated from a model which is corrected for position but these surps are not corrected for position. 

```


### HSP plots


##### Human plots

** Constructio**
```{r}


fit1_byconst <- posterior_samples(fit1_bayes) %>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num,
         diff = RT_obj - RT_subj)%>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(Position = "Verb")



fit2_byconst <- posterior_samples(fit2_bayes)%>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num,
         diff = RT_obj - RT_subj)%>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(Position = "Determiner")

fit3_byconst <- posterior_samples(fit3_bayes) %>%
  mutate(RT_subj  = b_Intercept,
         RT_obj = b_Intercept + b_Type_num,
         diff = RT_obj - RT_subj)%>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(Position = "Noun")


byconst <- dplyr::bind_rows(fit1_byconst, fit2_byconst, fit3_byconst) 

ggplot(byconst, aes(x = Position, y = mean, fill=Position)) + 
  geom_bar(stat="identity",position=position_dodge()) +
  geom_errorbar(aes(ymin=lower,ymax=upper),width=.2,position=position_dodge(.9)) + 
  scale_fill_manual(values = c("royalblue3","tan2","forestgreen"))+
  labs(x='Human data', y= 'Mean difference between ORC and SRC') +
  theme(axis.title=element_text(size=14,face="bold"),
        axis.text = element_text(size=14,face="bold"),
        legend.text = element_text(size=14),
        legend.title = element_text(size=14),
        legend.position="top")

ggsave('~/Downloads/rc_subset_byconst.png', width=5, height = 4)
```

** Item level**
```{r}

fit1_byitem <- split_by_randomeffect(fit1_bayes)%>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Verb') 


fit2_byitem <- split_by_randomeffect(fit2_bayes)%>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Determiner')

fit3_byitem <- split_by_randomeffect(fit3_bayes) %>%
  mutate(RT_SRC = b_Intercept + item_intercept,
         RT_ORC = RT_SRC + b_Type_num + item_slope,
         diff = RT_ORC - RT_SRC) %>%
  group_by(item) %>%
  summarise(mean = mean(diff),
            lower = quantile(diff, 0.025)[[1]],
            upper = quantile(diff, 0.975)[[1]]) %>%
  mutate(region = 'Noun')


byitem <- dplyr::bind_rows(fit1_byitem, fit2_byitem, fit3_byitem) 

ggplot(fit1_byitem, aes(x = reorder(item, mean), y=mean)) + 
  geom_point() +
  geom_errorbar(aes(ymin=lower,ymax=upper),width=.2,position=position_dodge(.9)) + 
  labs(x = 'Human data', y = 'Mean difference between ORC and SRC')


ggsave('~/Downloads/rc_subset_byitem.png', width=5.5, height = 4)

```

```{r}

surps_lstm <- read.csv("./Surprisals/data/gulordava/items_orc.lstm.csv") %>%
  mutate(word_pos = word_pos + 1,
         model = 'LSTM') %>%
  merge(rt.data,  by.y=c("Sentence", "WordPosition"), by.x=c("Sentence", "word_pos"))

surps_gpt2 <- read.csv("./Surprisals/data/gpt2/items_orc.gpt2.csv") %>%
  mutate(word_pos = word_pos + 1,
         model = 'GPT-2') %>%
  merge(rt.data,  by.y=c("Sentence", "WordPosition"), by.x=c("Sentence", "word_pos")) %>%
  rename(sum_surprisal = sumsurprisal)


surps <- dplyr::bind_rows(surps_lstm, surps_gpt2)

```


```{r}
surps_by_const <- surps %>%
  select(Type, sum_surprisal, mean_surprisal,item, model, ROI) %>%
  filter(ROI %in% c(0,1,2)) %>%
  group_by(model, Type, ROI) %>%
  summarise(sum_surprisal = mean(sum_surprisal)) %>%
  spread(Type, sum_surprisal) %>%
  mutate(surp_diff = RC_Obj - RC_Subj) %>%
  mutate(region = ifelse(ROI == 0, 'Verb', 
                         ifelse(ROI == 1, 'Determiner', 'Noun')))


ggplot(surps_by_const, aes(x=region, y=surp_diff, fill=region)) +
  geom_bar(stat="identity",position=position_dodge()) +
  # geom_errorbar(aes(ymin=lower,ymax=upper),width=.2,position=position_dodge(.9)) +
  scale_fill_manual(values = c("royalblue3","tan2","forestgreen"))+
  labs(x='Surprisal data', y= 'Mean surprisal difference\n between ORC and SRC') +
  theme(axis.title=element_text(size=14,face="bold"),
        axis.text = element_text(size=14,face="bold"),
        legend.text = element_text(size=14),
        legend.title = element_text(size=14),
        legend.position="top")

ggsave('~/Downloads/rc_subset_surp_byconst.png', width=5.5, height = 4)


```


```{r}

surps_byitem <- surps %>%
  select(Type, sum_surprisal, mean_surprisal,item, model, ROI) %>%
  filter(ROI == 0) %>%
  group_by(item, model, Type) %>%
  summarise(sum_surprisal = mean(sum_surprisal)) %>%
  spread(Type, sum_surprisal) %>%
  mutate(surp_diff = RC_Obj - RC_Subj) %>%
  merge(fit1_byitem, by='item')


ann_text <- surps_byitem %>%
  group_by(model) %>%
  summarise(cor = round(cor(surp_diff, mean), 2),
            lab = paste('r =', cor))

ggplot(surps_byitem, aes(x=surp_diff, y = mean)) + 
  geom_point() + 
  facet_wrap(~model, nrow = 2) + 
  labs(x = 'Surprisal difference between ORC and SRC', y = 'RT difference between ORC and SRC') + 
  geom_text(data=ann_text, 
            mapping = aes(x = -8, y=145,label=lab))
  
ggsave('~/Downloads/rc_subset_surpcor.png', width=5, height = 4)

```


### Surprisal lmer analysis

#### Loading in data


```{r}

spr <- read.csv("./preprocessed_data/Fillers.csv")
spr$Sentence <- str_replace_all(spr$Sentence, "%2C", ",")
spr <- spr %>% filter(RT<=7000) %>% rename(participant = MD5)

surps_lstm <- read.csv("./Surprisals/data/gulordava/items_filler.lstm.csv")
surps_gpt2 <- read.csv("./Surprisals/data/gpt2/items_filler.gpt2.csv")
#surps[surps$mean_surprisal == -1,]$mean_surprisal <- NA # 
#surps[surps$sum_surprisal == -1,]$sum_surprisal <- NA # recode NA surprisals as real NAs
surps_lstm$word_pos = surps_lstm$word_pos + 1# adjust to 1-indexing
surps_gpt2$word_pos = surps_gpt2$word_pos + 1# adjust to 1-indexing

# Load in frequencies from the Gulordava Wikipedia corpus
freqs <- read.csv("./freqs.csv")

```


```{r}
# merge the two dfs such that we have the relevant surprisal and frequency with each rt

spr$word <- tolower(spr$EachWord)
filler.freqs <- merge(x=spr, y=freqs, by.x="word", by.y="word", all.x=TRUE)

filler.surps <- merge(x=filler.freqs, y=surps_lstm, 
                      by.x=c("item", "WordPosition"), by.y=c("item.here", "word_pos"), all.x=TRUE)
filler.surps$surprisal_lstm = filler.surps$sum_surprisal # change to avg if that's more appropriate

filler.surps <- merge(x=filler.surps, y=surps_gpt2, 
                      by.x=c("item", "WordPosition"), by.y=c("item.here", "word_pos"), all.x=TRUE)
filler.surps$surprisal_gpt2 = filler.surps$sumsurprisal # change to avg if that's more appropriate

```


```{r}

# Store properties of past words in each row (going back 3 words)
filler.with_lags <- filler.surps %>% group_by_at(vars(item, participant)) %>%
                    mutate(RT_p1 = lag(RT), 
                           RT_p2 = lag(RT_p1), 
                           RT_p3 = lag(RT_p2),
                           length = nchar(EachWord),
                           length_p1 = lag(length), 
                           length_p2 = lag(length_p1),
                           length_p3 = lag(length_p2),
                           logfreq = log(count),
                           logfreq_p1 = lag(logfreq), 
                           logfreq_p2 = lag(logfreq_p1),
                           logfreq_p3 = lag(logfreq_p2),
                           surprisal_lstm_p1 = lag(surprisal_lstm),
                           surprisal_lstm_p2 = lag(surprisal_lstm_p1),
                           surprisal_lstm_p3 = lag(surprisal_lstm_p2),
                           surprisal_gpt2_p1 = lag(surprisal_gpt2),
                           surprisal_gpt2_p2 = lag(surprisal_gpt2_p1),
                           surprisal_gpt2_p3 = lag(surprisal_gpt2_p2)
                  )

```

```{r}

filler.drop.lstm <- subset(filler.with_lags, !is.na(surprisal_lstm) & !is.na(surprisal_lstm_p1) & 
                                        !is.na(surprisal_lstm_p2) & !is.na(surprisal_lstm_p3) &
                                        !is.na(logfreq) & !is.na(logfreq_p1) &
                                        !is.na(logfreq_p2) & !is.na(logfreq_p3))

filler.drop.gpt2 <- subset(filler.with_lags, !is.na(surprisal_gpt2) & !is.na(surprisal_gpt2_p1) & 
                                        !is.na(surprisal_gpt2_p2) & !is.na(surprisal_gpt2_p3) &
                                        !is.na(logfreq) & !is.na(logfreq_p1) &
                                        !is.na(logfreq_p2) & !is.na(logfreq_p3))

# print number of remaining rows
print(nrow(filler.with_lags))
print(nrow(filler.drop.lstm))
print(nrow(filler.drop.gpt2))


all_fillers = levels(as.factor(filler.with_lags$item))
print(length(all_fillers))
lstm_fillers = levels(as.factor(filler.drop.lstm$item))
print(length(lstm_fillers))
gpt2_fillers = levels(as.factor(filler.drop.gpt2$item))
print(length(gpt2_fillers))

#items that have been dropped
diff = setdiff(all_fillers, lstm_fillers)
print(diff)

filler.dropped <- subset(filler.with_lags, (is.na(surprisal_lstm) | is.na(surprisal_lstm_p1) | 
                                        is.na(surprisal_lstm_p2) | is.na(surprisal_lstm_p3) |
                                        is.na(logfreq) | is.na(logfreq_p1) |
                                        is.na(logfreq_p2) | is.na(logfreq_p3)) & (item %in% diff)) %>%
                  group_by(item, WordPosition) %>%
                  summarize(word = first(word),
                            sent = first(Sentence.x),
                           logfreq = first(logfreq),
                           logfreq_p1 = first(logfreq_p1), 
                           logfreq_p2 = first(logfreq_p2),
                           logfreq_p3 = first(logfreq_p3),
                           surprisal_lstm = first(surprisal_lstm),
                           surprisal_lstm_p1 = first(surprisal_lstm_p1),
                           surprisal_lstm_p2 = first(surprisal_lstm_p2),
                           surprisal_lstm_p3 = first(surprisal_lstm_p3))

filler.dropped

```

```{r}

models.filler.lstm <- lmer(data=filler.drop.lstm,
                      RT ~ surprisal_lstm + surprisal_lstm_p1 + surprisal_lstm_p2 + surprisal_lstm_p3 +
                           WordPosition + logfreq*length + logfreq_p1*length_p1 + 
                           logfreq_p2*length_p2 + logfreq_p3*length_p3 + (1 | participant) + (1 | item))
summary(models.filler.lstm) 

saveRDS(models.filler.lstm, "filler_lstm_sum.rds")

```


```{r}

models.filler.gpt2 <- lmer(data=filler.drop.gpt2,
                      RT ~ surprisal_gpt2 + surprisal_gpt2_p1 + surprisal_gpt2_p2 + surprisal_gpt2_p3 +
                           WordPosition + logfreq*length + logfreq_p1*length_p1 + 
                           logfreq_p2*length_p2 + logfreq_p3*length_p3 + (1 | participant) + (1 | item))
summary(models.filler.gpt2) 

saveRDS(models.filler.gpt2, "filler_gpt2_sum.rds")

```


#### Generating predicted RTs

```{r}


rc.surps.lstm <- read.csv("./Surprisals/data/gulordava/items_orc.lstm.csv")
# com_cols <- intersect(colnames(rc.surps.lstm_a), colnames(rc.surps.lstm_m))
# rc.surps.lstm <- rbind(rc.surps.lstm_a[,com_cols], rc.surps.lstm_m[,com_cols])


rc.surps.gpt2 <- read.csv("./Surprisals/data/gpt2/items_orc.gpt2.csv")
# rc.surps.gpt2_m <- read.csv("./Surprisals/data/gpt2/items_main.gpt2.csv")
# com_cols <- intersect(colnames(rc.surps.gpt2_a), colnames(rc.surps.gpt2_m))
# rc.surps.gpt2 <- rbind(rc.surps.gpt2_a[,com_cols], rc.surps.gpt2_m[,com_cols])

#rc.surps[rc.surps$surprisal == -1,]$surprisal <- NA # recode NA surprisals as real NAs
rc.surps.lstm$word_pos = rc.surps.lstm$word_pos + 1# adjust to 1-indexing
rc.surps.gpt2$word_pos = rc.surps.gpt2$word_pos + 1# adjust to 1-indexing

```


```{r}
## creating the right columns
rt.data <- read.csv("./preprocessed_data/RelativeClauseSet.csv", header=TRUE) %>%
  filter(RT <= 7000) %>%
  rename(participant = MD5) %>%
  mutate(Sentence = str_replace_all(Sentence, "%2C", ","))

rc.freqs <- merge(x=rt.data, y=freqs, by.x="EachWord", by.y="word", all.x=TRUE)
rc.surps <- merge(x=rc.freqs, y=rc.surps.lstm, 
                      by.x=c("Sentence", "WordPosition"), by.y=c("Sentence", "word_pos"), all.x=TRUE)
rc.surps$surprisal_lstm <- rc.surps$sum_surprisal

rc.surps <- merge(x=rc.surps, y=rc.surps.gpt2, 
                      by.x=c("Sentence", "WordPosition"), by.y=c("Sentence", "word_pos"), all.x=TRUE)
rc.surps$surprisal_gpt2 <- rc.surps$sumsurprisal


```


```{r}

rc.with_lags <-  rc.surps %>% group_by_at(vars(item, participant)) %>%
                    mutate(RT_p1 = lag(RT), 
                           RT_p2 = lag(RT_p1), 
                           RT_p3 = lag(RT_p2),
                           length = nchar(EachWord),
                           length_p1 = lag(length), 
                           length_p2 = lag(length_p1),
                           length_p3 = lag(length_p2),
                           logfreq = log(count),
                           logfreq_p1 = lag(logfreq), 
                           logfreq_p2 = lag(logfreq_p1),
                           logfreq_p3 = lag(logfreq_p2),
                           surprisal_lstm_p1 = lag(surprisal_lstm),
                           surprisal_lstm_p2 = lag(surprisal_lstm_p1),
                           surprisal_lstm_p3 = lag(surprisal_lstm_p2),
                           surprisal_gpt2_p1 = lag(surprisal_gpt2),
                           surprisal_gpt2_p2 = lag(surprisal_gpt2_p1),
                           surprisal_gpt2_p3 = lag(surprisal_gpt2_p2)
                  ) %>% subset(ROI %in% c(0, 1, 2)) %>%
                    mutate(position=droplevels(as.factor(ROI)))


```


```{r}

rc.drop.lstm <- subset(rc.with_lags, !is.na(surprisal_lstm) & !is.na(surprisal_lstm_p1) & 
                                        !is.na(surprisal_lstm_p2) & !is.na(surprisal_lstm_p3) &
                                        !is.na(logfreq) & !is.na(logfreq_p1) &
                                        !is.na(logfreq_p2) & !is.na(logfreq_p3))

rc.drop.gpt2 <- subset(rc.with_lags, !is.na(surprisal_gpt2) & !is.na(surprisal_gpt2_p1) & 
                                        !is.na(surprisal_gpt2_p2) & !is.na(surprisal_gpt2_p3) &
                                        !is.na(logfreq_p2) & !is.na(logfreq_p3))


# print number of remaining rows
print(nrow(rc.with_lags))
print(nrow(rc.drop.lstm))
print(nrow(rc.drop.gpt2))

```


```{r}


generate_predicted_dat <- function(filler_model, filler_dat, rc_dat){
  filler_dat$predicted <- predict(filler_model, newdata=filler_dat, allow.new.levels = TRUE)
  
  wordpos_model <- lmer(predicted ~ scale(WordPosition) +
                            (1 +scale(WordPosition) | participant),
                            data =filler_dat)
  
  rc_dat$predicted <- predict(filler_model, newdata=rc_dat, allow.new.levels = TRUE)
  
  rc_dat$pred_rt_wordpos <- predict(wordpos_model, rc_dat)
  rc_dat$corrected_predicted <- rc_dat$predicted - rc_dat$pred_rt_wordpos
  
  return(rc_dat)
  
}


rc.drop.lstm <- generate_predicted_dat(models.filler.lstm, filler.drop.lstm, rc.drop.lstm)
rc.drop.gpt2 <- generate_predicted_dat(models.filler.gpt2, filler.drop.gpt2, rc.drop.gpt2)



#rc.drop.lstm$predicted <- predict(models.filler.lstm, newdata=rc.drop.lstm, allow.new.levels = TRUE)
#contrasts(agree.drop.lstm$position) <- contr.sum(3)/2
#saveRDS(agree.drop.lstm, "agreement_data.lstm.rds")

#contrasts(agree.drop.gpt2$position) <- contr.sum(3)/2
#rc.drop.gpt2$predicted <- predict(models.filler.gpt2, newdata=rc.drop.gpt2, allow.new.levels = TRUE)
#saveRDS(agree.drop.gpt2, "agreement_data.gpt2.rds")

```


```{r}

rc.drop.lstm <- rc.drop.lstm %>%
  select(item, participant, Type, ROI, corrected_predicted) %>%
  mutate(model = 'LSTM') %>% 
  rename(corrected_rt = corrected_predicted)

rc.drop.gpt2 <- rc.drop.gpt2 %>%
  select(item, participant, Type, ROI, corrected_predicted) %>%
  mutate(model = 'GPT-2') %>% 
  rename(corrected_rt = corrected_predicted)




rt.data$wordpos_predrt <- predict(position_fit_lmer_nocor, rt.data)
rt.data$corrected_rt <- rt.data$RT - rt.data$wordpos_predrt

rt.data_curr <- rt.data %>%
  select(item, participant, Type, ROI, corrected_rt) %>%
  mutate(model = 'Human')

predicted_dat <- rbind(rc.drop.lstm, rc.drop.gpt2, rt.data_curr)


```


## Plot construction level effects

```{r}

predicted_dat_summ <- predicted_dat %>%
  filter(ROI %in% c(0,1,2)) %>%
  mutate(ROI = ifelse(ROI == 0, 'Verb', ifelse(ROI == 1, 'Determiner', 'Noun'))) %>%
  group_by(Type, ROI, model) %>%
  summarise(mean_predicted = mean(corrected_rt, na.rm=TRUE),
            se_predicted = sd(corrected_rt, na.rm=TRUE)/sqrt(n()),
            lower = mean_predicted - 2*se_predicted,
            upper = mean_predicted + 2*se_predicted) %>%
  mutate(model = factor(model, levels = c('Human', 'GPT-2', 'LSTM')))



ggplot(predicted_dat_summ, aes(x = Type, y = mean_predicted, fill=ROI)) + 
  geom_bar(stat="identity", position= "dodge") + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper),
                width=0.2,
                position=position_dodge(0.9)) +
  facet_grid(~model)


```




#### Possibly cleaner functions for the future
```{r}

# create_merged_dat <- function(rt_dat, surp_dat, freqs){
#   merged = rt_dat %>%
#     merge(freqs, by.x="EachWord", by.y="word", all.x=TRUE) %>%
#     merge(surp_dat, by.x=c("Sentence", "WordPosition"), by.y=c("Sentence", "word_pos"), all.x=TRUE) %>%
#     group_by_at(vars(participant, Sentence)) %>%
#     mutate(RT_p1 = lag(RT), 
#            RT_p2 = lag(RT_p1), 
#            RT_p3 = lag(RT_p2),
#            length = nchar(EachWord),
#            length_p1 = lag(length), 
#            length_p2 = lag(length_p1),
#            length_p3 = lag(length_p2),
#            logfreq = log(count),
#            logfreq_p1 = lag(logfreq), 
#            logfreq_p2 = lag(logfreq_p1),
#            logfreq_p3 = lag(logfreq_p2),
#            surprisal_p1 = lag(surprisal),
#            surprisal_p2 = lag(surprisal_p1),
#            surprisal_p3 = lag(surprisal_p2))
#                            
#   merged_dropped = subset(merged, 
#                           !is.na(surprisal) & !is.na(surprisal_p1)& 
#                           !is.na(surprisal_p2) & !is.na(surprisal_p3) &
#                           !is.na(logfreq) & !is.na(logfreq_p1) &
#                           !is.na(logfreq_p2) & !is.na(logfreq_p3))
#   
#   print(paste(nrow(merged), nrow(merged_dropped)))
#   
#   return(merged_dropped)
#     
# }

```

```{r, cache=TRUE}

# surp_files_fillers <- c('./Surprisals/data/gulordava/items_filler.lstm.csv',
#                 './Surprisals/data/gpt2/items_filler.gpt2.csv',
#                 './Surprisals/data/rnng/items_filler.rnng.csv')
# 
# 
# models <- c('lstm', 'gpt2', 'rnng')
# 
# 
# rt.data$Sentence <- str_replace_all(rt.data$Sentence, "%2C", ",")
# filler.data$Sentence <- str_replace_all(filler.data$Sentence, "%2C", ",")
# 
# freqs <- read.csv("./freqs.csv")
# 
# predicted_dat_list <- list()
# filler_surp_models <- list()
# 
# i <- 1
# 
# for(model_name in models){
#   
#   folder_name = ifelse(model_name == 'lstm', 'gulordava', model_name)
#   rc_surp_name <- paste('./Surprisals/data/', folder_name, '/items_orc.', model_name, '.csv', sep = '')
#   filler_surp_name <- paste('./Surprisals/data/', folder_name,'/items_filler.', model_name, '.csv', sep = '')
#   
#   ## Load in surprisal
#   curr_rc_surp <- read.csv(rc_surp_name) %>%
#     mutate(model = model_name,
#            surprisal = ifelse(surprisal == -1, NA, surprisal),
#            word_pos = word_pos + 1,
#            Sentence = str_replace_all(Sentence, "%2C", ","))
#   
#   curr_filler_surp <- read.csv(filler_surp_name) %>%
#     mutate(model = model_name,
#            surprisal = ifelse(mean_surprisal == -1, NA, mean_surprisal),
#            word_pos = word_pos + 1,
#            Sentence = str_replace_all(Sentence, "%2C", ","))
#   
#   ## Merge surprisal with RTs
#   curr_rc_dat <- create_merged_dat(rt.data, curr_rc_surp, freqs)
#   curr_filler_dat <- create_merged_dat(filler.data, curr_filler_surp, freqs)
#   
#   ## Fit filler model
#   curr_filler_model <- lmer(data=curr_filler_dat,
#                             RT ~ surprisal + surprisal_p1 + surprisal_p2 + surprisal_p3 +
#                             WordPosition + 
#                             logfreq*length + logfreq_p1*length_p1 + logfreq_p2*length_p2 + logfreq_p3*length_p3 +
#                             (1 | participant) + (1 | item))
#   
#   #saveRDS(curr_filler_model, paste('./saved_objects/filler_lm_', model_name, sep=''))
#   
#   ## Generated predicted RT
#   curr_rc_dat$pred_rt <- predict(curr_filler_model,
#                                    newdata=curr_rc_dat,
#                                    allow.new.levels=TRUE)
#   
#   curr_filler_dat$pred_rt <- predict(curr_filler_model,
#                                    newdata=curr_filler_dat)
#   
#   
#   ## Correct predicted RT for word position
#   
#   curr_wordpos_model <- lmer(pred_rt ~ scale(WordPosition) +
#                             (1 +scale(WordPosition) | participant),
#                             data = curr_filler_dat)
#   
#   
#   
#   curr_rc_dat$pred_rt_wordpos <- predict(curr_wordpos_model, curr_rc_dat)
#   curr_rc_dat$corrected_pred_rt <- curr_rc_dat$pred_rt - curr_rc_dat$pred_rt_wordpos
#   
#   curr_rc_dat$model <- model_name
# 
#   predicted_dat_list[[i]] <- curr_rc_dat
# 
#   i <- i+1
# 
# }
# 
# predicted_dat <- dplyr::bind_rows(predicted_dat_list)

```



### Fit BRMS models at the verb position


```{r, cache=TRUE}

all_fit1_byitem <- list()

i <- 1

for(model_name in unique(predicted_dat$model)){
  print(model_name)
  curr_verb_dat = predicted_dat %>%
    filter(model == model_name) %>%
    filter(ROI == 0) %>%
    mutate(Type = factor(Type, levels = c('RC_Subj', 'RC_Obj')),
         Type_num = ifelse(Type == 'RC_Subj', 0, 1))

  # curr_fit1_bayes <- brm(corrected_pred_rt ~ Type_num + (0 + Type_num | participant) + (1 + Type_num | item),
  #                         data=curr_verb_dat,
  #                         prior = prior1,
  #                         cores = 4,
  #                         iter = 4000,
  #                         seed = 117
  #                       )
  # 
  # saveRDS(curr_fit1_bayes, paste('./saved_objects/fit1_bayes_prior1_surp_', model_name, sep=''))
  
  curr_fit1_bayes = readRDS(paste('./saved_objects/fit1_bayes_prior1_surp_', model_name, sep=''))


  curr_fit1_byitem <- split_by_randomeffect(curr_fit1_bayes)%>%
    mutate(RT_SRC = b_Intercept + item_intercept,
           RT_ORC = RT_SRC + b_Type_num + item_slope,
           diff = RT_ORC - RT_SRC) %>%
    group_by(item) %>%
    summarise(mean_surp = mean(diff),
              lower_surp = quantile(diff, 0.025)[[1]],
              upper_surp = quantile(diff, 0.975)[[1]]) %>%
    mutate(region = 'Verb') %>%
    mutate(model = model_name)

  all_fit1_byitem[[i]] <- curr_fit1_byitem
  i <- i+1

}


```


#### Structure level effects

```{r}

all_bystruc_surp <- list()

i <- 1
for(model_name in unique(predicted_dat$model)){
  print(model_name)
  curr_fit1_bayes <- readRDS(paste('./saved_objects/fit1_bayes_prior1_surp_', model_name, sep=''))


  curr_fit1_samples_summ <- posterior_samples(curr_fit1_bayes) %>%
    mutate(RT_subj  = b_Intercept,
           RT_obj = b_Intercept + b_Type_num) %>%
    select(RT_subj, RT_obj) %>%
    gather(key = 'cond', value = 'RT', RT_subj, RT_obj) %>%
    group_by(cond) %>%
    summarise(mean_surp = mean(RT),
              lower_surp = quantile(RT, 0.025)[[1]],
              upper_surp = quantile(RT, 0.975)[[1]]) %>%
    mutate(region = 'Verb',
           model = model_name) 
  
  all_bystruc_surp[[i]] <- curr_fit1_samples_summ
  i <- i+1

}

## Plot surprisal predictions for each structure

all_bystruc_surp <- dplyr::bind_rows(all_bystruc_surp)%>%
  merge(fit1_samples_summ, by = c('region', 'cond')) %>%
  gather(key = 'dv_type', value = 'mean', mean, mean_surp) %>%
  mutate(upper = ifelse(dv_type == 'mean', upper, upper_surp),
         lower = ifelse(dv_type == 'mean', lower, lower_surp))

ggplot(all_bystruc_surp, aes(x=dv_type, y=mean, fill=cond)) +
  geom_bar(stat="identity", position= "dodge") + 
  geom_errorbar(aes(ymin=lower,
                    ymax=upper),
                width=0.2,
                position=position_dodge(0.9)) +
  facet_wrap(~model)

```

#### Item level effects
```{r}

all_surp_fit1_byitem <- dplyr::bind_rows(all_fit1_byitem) 

## Plot surprisal predictions for each item

ggplot(all_surp_fit1_byitem, aes(y = mean_surp, x = item)) + 
  geom_point() + 
  facet_grid(~model) + 
  geom_errorbar(aes(ymin=lower_surp,
                    ymax=upper_surp), 
                width=0.2) + 
  labs(y = 'RT(ORC) - RT(SRC)', x = 'Surp(ORC) - Surp(SRC)') +
  geom_smooth(method = 'lm')

```


```{r}

## Plot correlation of suprisal predictions with raw RTs

all_byitem_surp <- merge(all_split, all_surp_fit1_byitem, by = c('region', 'item'))


ggplot(all_byitem_surp, aes(y = mean, x = mean_surp)) + 
  geom_point() + 
  facet_grid(~model) + 
  # geom_errorbar(aes(ymin=lower,
  #                   ymax=upper), 
  #               width=0.2) + 
  labs(y = 'Raw RT(ORC) - Raw RT(SRC)', x = 'Pred RT(ORC) - Pred RT(SRC)') +
  geom_smooth(method = 'lm')

```


#### Looking at summary of BRMS models

```{r}

for(model_name in unique(predicted_dat$model)){
  print(model_name)
  curr_fit1_bayes <- readRDS(paste('./saved_objects/fit1_bayes_prior1_surp_', model_name, sep=''))
  print(summary(curr_fit1_bayes))
}

```


#### Comparing by-item predictions from BRMS and LMER models


```{r, cache=TRUE}


all_fit1_byitem_lmer <- list()

i <- 1
for(model_name in unique(predicted_dat$model)){
  print(model_name)
  
  curr_verb_dat = predicted_dat %>%
    filter(model == model_name) %>%
    filter(ROI == 0) %>%
    mutate(Type = factor(Type, levels = c('RC_Subj', 'RC_Obj')),
         Type_num = ifelse(Type == 'RC_Subj', 0, 1))

  curr_fit1_lmer <- lmer(corrected_pred_rt ~ Type_num + 
                           (0 + Type_num | participant) + 
                           (1 + Type_num | item),
                         data=curr_verb_dat
                        )
  
  curr_item_df <- data.frame(ranef(curr_fit1_lmer)) %>%
    filter(grpvar == 'item') %>%
    select(!condsd) %>%
    spread(key = term, value = condval) %>%
    rename(item = grp,
           mean_surp_lmer = Type_num) %>%
    mutate(model = model_name)
  
  all_fit1_byitem_lmer[[i]] <- curr_item_df
  i <- i+1
  
}

all_surp_fit1_byitem_lmer <- dplyr::bind_rows(all_fit1_byitem_lmer)

```




```{r}

all_byitem_surp2 <- merge(all_surp_fit1_byitem,all_surp_fit1_byitem_lmer, by = c('model', 'item'))


ggplot(all_byitem_surp2, aes(y = mean_surp_lmer, x = mean_surp)) + 
  geom_point() + 
  facet_grid(~model) + 
  # geom_errorbar(aes(ymin=lower,
  #                   ymax=upper), 
  #               width=0.2) + 
  labs(y = 'LMER Pred RT(ORC) - Pred RT(SRC)', x = 'BRMS Pred RT(ORC) - Pred RT(SRC)') +
  geom_smooth(method = 'lm')


```



#### Creating a CSV of by-item surprisal and RTs


```{r}

by_item <- all_byitem_surp %>%
  select(item, model, mean, mean_surp) %>%
  spread(key=model, value=mean_surp) %>%
  rename(human = mean)


sent_dat <- verb_dat %>%
  filter(Type == 'RC_Subj') %>%
  select(item, Sentence, EachWord) %>%
  distinct() %>%
  merge(freqs, by.x="EachWord", by.y="word") %>%
  rename(Verb = EachWord,
         Frequency = count)

by_item <- merge(sent_dat, by_item, by = 'item')

write.csv(by_item, 'RC_subset_byitem_RT_surps.csv')

```




